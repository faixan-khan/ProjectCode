| distributed init (rank 0): env://, gpu 0
| distributed init (rank 3): env://, gpu 3
| distributed init (rank 1): env://, gpu 1
| distributed init (rank 2): env://, gpu 2
[22:03:47.086886] job dir: /home/khanff/cvpr23/LoMaR
[22:03:47.087171] Namespace(MASTER_ADDR='localhost',
MASTER_PORT='10019',
accum_iter=4,
batch_size=256,
blr=0.00015,
data_path='/ibex/ai/reference/CV/ILSVR/classification-localization/data/jpeg',
device='cuda',
dist_backend='nccl',
dist_on_itp=False,
dist_url='env://',
distributed=True,
epochs=100,
gpu=0,
input_size=224,
local_rank=0,
log_dir='/ibex/ai/project/c2090/lomar_plus_save/logs/linear_mae_encoderonly_mask_0.8_wind_7_num_2_epochs_100',
lr=None,
mask_ratio=0.8,
min_lr=0.0,
model='mae_vit_base_patch16',
norm_pix_loss=True,
num_window=2,
num_workers=10,
output_dir='/ibex/ai/project/c2090/lomar_plus_save/checkpoint/linear_mae_encoderonly_mask_0.8_wind_7_num_2_epochs_100',
pin_mem=True,
rank=0,
resume='',
seed=0,
start_epoch=0,
warmup_epochs=5,
weight_decay=0.05,
window_size=7,
world_size=4)
[22:04:06.618932] Dataset ImageFolder
    Number of datapoints: 1281167
    Root location: /ibex/ai/reference/CV/ILSVR/classification-localization/data/jpeg/train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(224, 224), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
           )
[22:04:06.619391] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x2baaf9442e80>
[22:04:08.626898] Model = MaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
  )
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (Linear_attention): FullAttention(
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (linear_attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=768, out_features=768, bias=True)
          (key_projection): Linear(in_features=768, out_features=768, bias=True)
          (value_projection): Linear(in_features=768, out_features=768, bias=True)
          (out_projection): Linear(in_features=768, out_features=768, bias=True)
        )
        (query_projection): Linear(in_features=768, out_features=768, bias=True)
        (key_projection): Linear(in_features=768, out_features=768, bias=True)
        (value_projection): Linear(in_features=768, out_features=768, bias=True)
        (out_projection): Linear(in_features=768, out_features=768, bias=True)
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (rpe_k): iRPE(head_dim=64, num_heads=1, mode="contextual", method=3, transposed=True, num_buckets=50, initializer=<function iRPE.__init__.<locals>.initializer at 0x2bab8f137950>, rpe_config={'shared_head': True, 'mode': 'contextual', 'method': 3, 'alpha': 1.9, 'beta': 3.8, 'gamma': 15.2, 'num_buckets': 50})
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (Linear_attention): FullAttention(
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (linear_attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=768, out_features=768, bias=True)
          (key_projection): Linear(in_features=768, out_features=768, bias=True)
          (value_projection): Linear(in_features=768, out_features=768, bias=True)
          (out_projection): Linear(in_features=768, out_features=768, bias=True)
        )
        (query_projection): Linear(in_features=768, out_features=768, bias=True)
        (key_projection): Linear(in_features=768, out_features=768, bias=True)
        (value_projection): Linear(in_features=768, out_features=768, bias=True)
        (out_projection): Linear(in_features=768, out_features=768, bias=True)
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (rpe_k): iRPE(head_dim=64, num_heads=1, mode="contextual", method=3, transposed=True, num_buckets=50, initializer=<function iRPE.__init__.<locals>.initializer at 0x2bab95ea1d08>, rpe_config={'shared_head': True, 'mode': 'contextual', 'method': 3, 'alpha': 1.9, 'beta': 3.8, 'gamma': 15.2, 'num_buckets': 50})
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (Linear_attention): FullAttention(
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (linear_attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=768, out_features=768, bias=True)
          (key_projection): Linear(in_features=768, out_features=768, bias=True)
          (value_projection): Linear(in_features=768, out_features=768, bias=True)
          (out_projection): Linear(in_features=768, out_features=768, bias=True)
        )
        (query_projection): Linear(in_features=768, out_features=768, bias=True)
        (key_projection): Linear(in_features=768, out_features=768, bias=True)
        (value_projection): Linear(in_features=768, out_features=768, bias=True)
        (out_projection): Linear(in_features=768, out_features=768, bias=True)
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (rpe_k): iRPE(head_dim=64, num_heads=1, mode="contextual", method=3, transposed=True, num_buckets=50, initializer=<function iRPE.__init__.<locals>.initializer at 0x2bab95eb8158>, rpe_config={'shared_head': True, 'mode': 'contextual', 'method': 3, 'alpha': 1.9, 'beta': 3.8, 'gamma': 15.2, 'num_buckets': 50})
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (Linear_attention): FullAttention(
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (linear_attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=768, out_features=768, bias=True)
          (key_projection): Linear(in_features=768, out_features=768, bias=True)
          (value_projection): Linear(in_features=768, out_features=768, bias=True)
          (out_projection): Linear(in_features=768, out_features=768, bias=True)
        )
        (query_projection): Linear(in_features=768, out_features=768, bias=True)
        (key_projection): Linear(in_features=768, out_features=768, bias=True)
        (value_projection): Linear(in_features=768, out_features=768, bias=True)
        (out_projection): Linear(in_features=768, out_features=768, bias=True)
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (rpe_k): iRPE(head_dim=64, num_heads=1, mode="contextual", method=3, transposed=True, num_buckets=50, initializer=<function iRPE.__init__.<locals>.initializer at 0x2bab95ecd510>, rpe_config={'shared_head': True, 'mode': 'contextual', 'method': 3, 'alpha': 1.9, 'beta': 3.8, 'gamma': 15.2, 'num_buckets': 50})
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (Linear_attention): FullAttention(
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (linear_attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=768, out_features=768, bias=True)
          (key_projection): Linear(in_features=768, out_features=768, bias=True)
          (value_projection): Linear(in_features=768, out_features=768, bias=True)
          (out_projection): Linear(in_features=768, out_features=768, bias=True)
        )
        (query_projection): Linear(in_features=768, out_features=768, bias=True)
        (key_projection): Linear(in_features=768, out_features=768, bias=True)
        (value_projection): Linear(in_features=768, out_features=768, bias=True)
        (out_projection): Linear(in_features=768, out_features=768, bias=True)
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (rpe_k): iRPE(head_dim=64, num_heads=1, mode="contextual", method=3, transposed=True, num_buckets=50, initializer=<function iRPE.__init__.<locals>.initializer at 0x2bab95ee18c8>, rpe_config={'shared_head': True, 'mode': 'contextual', 'method': 3, 'alpha': 1.9, 'beta': 3.8, 'gamma': 15.2, 'num_buckets': 50})
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (Linear_attention): FullAttention(
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (linear_attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=768, out_features=768, bias=True)
          (key_projection): Linear(in_features=768, out_features=768, bias=True)
          (value_projection): Linear(in_features=768, out_features=768, bias=True)
          (out_projection): Linear(in_features=768, out_features=768, bias=True)
        )
        (query_projection): Linear(in_features=768, out_features=768, bias=True)
        (key_projection): Linear(in_features=768, out_features=768, bias=True)
        (value_projection): Linear(in_features=768, out_features=768, bias=True)
        (out_projection): Linear(in_features=768, out_features=768, bias=True)
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (rpe_k): iRPE(head_dim=64, num_heads=1, mode="contextual", method=3, transposed=True, num_buckets=50, initializer=<function iRPE.__init__.<locals>.initializer at 0x2bab95ef5c80>, rpe_config={'shared_head': True, 'mode': 'contextual', 'method': 3, 'alpha': 1.9, 'beta': 3.8, 'gamma': 15.2, 'num_buckets': 50})
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (Linear_attention): FullAttention(
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (linear_attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=768, out_features=768, bias=True)
          (key_projection): Linear(in_features=768, out_features=768, bias=True)
          (value_projection): Linear(in_features=768, out_features=768, bias=True)
          (out_projection): Linear(in_features=768, out_features=768, bias=True)
        )
        (query_projection): Linear(in_features=768, out_features=768, bias=True)
        (key_projection): Linear(in_features=768, out_features=768, bias=True)
        (value_projection): Linear(in_features=768, out_features=768, bias=True)
        (out_projection): Linear(in_features=768, out_features=768, bias=True)
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (rpe_k): iRPE(head_dim=64, num_heads=1, mode="contextual", method=3, transposed=True, num_buckets=50, initializer=<function iRPE.__init__.<locals>.initializer at 0x2bab95f0b0d0>, rpe_config={'shared_head': True, 'mode': 'contextual', 'method': 3, 'alpha': 1.9, 'beta': 3.8, 'gamma': 15.2, 'num_buckets': 50})
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (Linear_attention): FullAttention(
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (linear_attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=768, out_features=768, bias=True)
          (key_projection): Linear(in_features=768, out_features=768, bias=True)
          (value_projection): Linear(in_features=768, out_features=768, bias=True)
          (out_projection): Linear(in_features=768, out_features=768, bias=True)
        )
        (query_projection): Linear(in_features=768, out_features=768, bias=True)
        (key_projection): Linear(in_features=768, out_features=768, bias=True)
        (value_projection): Linear(in_features=768, out_features=768, bias=True)
        (out_projection): Linear(in_features=768, out_features=768, bias=True)
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (rpe_k): iRPE(head_dim=64, num_heads=1, mode="contextual", method=3, transposed=True, num_buckets=50, initializer=<function iRPE.__init__.<locals>.initializer at 0x2bab95f21488>, rpe_config={'shared_head': True, 'mode': 'contextual', 'method': 3, 'alpha': 1.9, 'beta': 3.8, 'gamma': 15.2, 'num_buckets': 50})
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (Linear_attention): FullAttention(
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (linear_attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=768, out_features=768, bias=True)
          (key_projection): Linear(in_features=768, out_features=768, bias=True)
          (value_projection): Linear(in_features=768, out_features=768, bias=True)
          (out_projection): Linear(in_features=768, out_features=768, bias=True)
        )
        (query_projection): Linear(in_features=768, out_features=768, bias=True)
        (key_projection): Linear(in_features=768, out_features=768, bias=True)
        (value_projection): Linear(in_features=768, out_features=768, bias=True)
        (out_projection): Linear(in_features=768, out_features=768, bias=True)
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (rpe_k): iRPE(head_dim=64, num_heads=1, mode="contextual", method=3, transposed=True, num_buckets=50, initializer=<function iRPE.__init__.<locals>.initializer at 0x2bab95f33840>, rpe_config={'shared_head': True, 'mode': 'contextual', 'method': 3, 'alpha': 1.9, 'beta': 3.8, 'gamma': 15.2, 'num_buckets': 50})
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (Linear_attention): FullAttention(
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (linear_attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=768, out_features=768, bias=True)
          (key_projection): Linear(in_features=768, out_features=768, bias=True)
          (value_projection): Linear(in_features=768, out_features=768, bias=True)
          (out_projection): Linear(in_features=768, out_features=768, bias=True)
        )
        (query_projection): Linear(in_features=768, out_features=768, bias=True)
        (key_projection): Linear(in_features=768, out_features=768, bias=True)
        (value_projection): Linear(in_features=768, out_features=768, bias=True)
        (out_projection): Linear(in_features=768, out_features=768, bias=True)
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (rpe_k): iRPE(head_dim=64, num_heads=1, mode="contextual", method=3, transposed=True, num_buckets=50, initializer=<function iRPE.__init__.<locals>.initializer at 0x2bab95f47bf8>, rpe_config={'shared_head': True, 'mode': 'contextual', 'method': 3, 'alpha': 1.9, 'beta': 3.8, 'gamma': 15.2, 'num_buckets': 50})
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (Linear_attention): FullAttention(
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (linear_attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=768, out_features=768, bias=True)
          (key_projection): Linear(in_features=768, out_features=768, bias=True)
          (value_projection): Linear(in_features=768, out_features=768, bias=True)
          (out_projection): Linear(in_features=768, out_features=768, bias=True)
        )
        (query_projection): Linear(in_features=768, out_features=768, bias=True)
        (key_projection): Linear(in_features=768, out_features=768, bias=True)
        (value_projection): Linear(in_features=768, out_features=768, bias=True)
        (out_projection): Linear(in_features=768, out_features=768, bias=True)
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (rpe_k): iRPE(head_dim=64, num_heads=1, mode="contextual", method=3, transposed=True, num_buckets=50, initializer=<function iRPE.__init__.<locals>.initializer at 0x2bab95f5e048>, rpe_config={'shared_head': True, 'mode': 'contextual', 'method': 3, 'alpha': 1.9, 'beta': 3.8, 'gamma': 15.2, 'num_buckets': 50})
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (Linear_attention): FullAttention(
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (linear_attention): AttentionLayer(
          (inner_attention): FullAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (query_projection): Linear(in_features=768, out_features=768, bias=True)
          (key_projection): Linear(in_features=768, out_features=768, bias=True)
          (value_projection): Linear(in_features=768, out_features=768, bias=True)
          (out_projection): Linear(in_features=768, out_features=768, bias=True)
        )
        (query_projection): Linear(in_features=768, out_features=768, bias=True)
        (key_projection): Linear(in_features=768, out_features=768, bias=True)
        (value_projection): Linear(in_features=768, out_features=768, bias=True)
        (out_projection): Linear(in_features=768, out_features=768, bias=True)
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
        (rpe_k): iRPE(head_dim=64, num_heads=1, mode="contextual", method=3, transposed=True, num_buckets=50, initializer=<function iRPE.__init__.<locals>.initializer at 0x2bab95f71400>, rpe_config={'shared_head': True, 'mode': 'contextual', 'method': 3, 'alpha': 1.9, 'beta': 3.8, 'gamma': 15.2, 'num_buckets': 50})
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (encoder_pred): Linear(in_features=768, out_features=768, bias=True)
)
[22:04:08.626983] base lr: 1.50e-04
[22:04:08.627001] actual lr: 2.40e-03
[22:04:08.627014] accumulate grad iterations: 4
[22:04:08.627025] effective batch size: 4096
[22:04:08.627037] True
[22:04:08.627048] 0
[22:04:08.663877] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    eps: 1e-08
    lr: 0.0024
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    eps: 1e-08
    lr: 0.0024
    weight_decay: 0.05
)
[22:04:08.664021] Start training for 100 epochs
[22:04:08.665218] log_dir: /ibex/ai/project/c2090/lomar_plus_save/logs/linear_mae_encoderonly_mask_0.8_wind_7_num_2_epochs_100
[22:04:19.251495] Loss is nan, stopping training
