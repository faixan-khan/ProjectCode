| distributed init (rank 1): env://, gpu 1
| distributed init (rank 0): env://, gpu 0
| distributed init (rank 2): env://, gpu 2
| distributed init (rank 3): env://, gpu 3
[00:25:12.142127] job dir: /home/khanff/cvpr23/lomar
[00:25:12.142227] Namespace(MASTER_ADDR='localhost',
MASTER_PORT='10019',
accum_iter=4,
amp_autocast=True,
batch_size=256,
blr=0.00015,
data_path='/ibex/ai/reference/CV/ILSVR/classification-localization/data/jpeg',
device='cuda',
dist_backend='nccl',
dist_on_itp=False,
dist_url='env://',
distributed=True,
epochs=100,
gpu=0,
input_size=384,
local_rank=0,
log_dir='/ibex/ai/project/c2090/lomar_plus_save/logs/raven/mae_encoderonly_mask_0.8_neigh_5_wind_9_num_4_epoches_100_r_384',
lr=None,
mask_ratio=0.8,
min_lr=0.0,
model='mae_vit_base_patch16_384',
neigh_ratio=0.05,
norm_pix_loss=True,
num_window=4,
num_workers=10,
output_dir='/ibex/ai/project/c2090/lomar_plus_save/checkpoint/raven/mae_encoderonly_mask_0.8_neigh_5_wind_9_num_4_epochs_100_r_384',
pin_mem=True,
rank=0,
resume='',
seed=0,
start_epoch=0,
warmup_epochs=5,
weight_decay=0.05,
window_size=9,
world_size=4)
[00:25:35.975804] Dataset ImageFolder
    Number of datapoints: 1281167
    Root location: /ibex/ai/reference/CV/ILSVR/classification-localization/data/jpeg/train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(384, 384), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
           )
[00:25:36.215114] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x2b6649462b70>
[00:25:37.155519] Model = MaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
  )
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (encoder_pred): Linear(in_features=768, out_features=768, bias=True)
)
[00:25:37.155584] base lr: 1.50e-04
[00:25:37.155595] actual lr: 2.40e-03
[00:25:37.155603] accumulate grad iterations: 4
[00:25:37.155611] effective batch size: 4096
[00:25:37.155617] True
[00:25:37.155624] 0
[00:25:37.180507] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    eps: 1e-08
    lr: 0.0024
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    eps: 1e-08
    lr: 0.0024
    weight_decay: 0.05
)
[00:25:37.180576] Start training for 100 epochs
[00:25:37.181107] log_dir: /ibex/ai/project/c2090/lomar_plus_save/logs/raven/mae_encoderonly_mask_0.8_neigh_5_wind_9_num_4_epoches_100_r_384
[00:25:52.794605] Epoch: [0]  [   0/1251]  eta: 5:25:31  lr: 0.000000  loss: 1.9144 (1.9144)  time: 15.6125  data: 8.4851  max mem: 54028
[00:26:11.602629] Epoch: [0]  [  20/1251]  eta: 0:33:37  lr: 0.000008  loss: 1.8591 (1.8356)  time: 0.9403  data: 0.0032  max mem: 55008
[00:26:30.014715] Epoch: [0]  [  40/1251]  eta: 0:26:00  lr: 0.000015  loss: 1.4522 (1.6517)  time: 0.9206  data: 0.0006  max mem: 55008
[00:26:48.498367] Epoch: [0]  [  60/1251]  eta: 0:23:12  lr: 0.000023  loss: 1.2297 (1.5133)  time: 0.9241  data: 0.0006  max mem: 55008
[00:27:06.913015] Epoch: [0]  [  80/1251]  eta: 0:21:37  lr: 0.000031  loss: 1.0939 (1.4120)  time: 0.9207  data: 0.0006  max mem: 55008
[00:27:25.544954] Epoch: [0]  [ 100/1251]  eta: 0:20:34  lr: 0.000038  loss: 1.0353 (1.3377)  time: 0.9309  data: 0.0006  max mem: 55008
[00:27:44.089866] Epoch: [0]  [ 120/1251]  eta: 0:19:46  lr: 0.000046  loss: 0.9911 (1.2805)  time: 0.9272  data: 0.0006  max mem: 55008
[00:28:02.626039] Epoch: [0]  [ 140/1251]  eta: 0:19:05  lr: 0.000054  loss: 0.9580 (1.2344)  time: 0.9267  data: 0.0040  max mem: 55008
[00:28:21.161550] Epoch: [0]  [ 160/1251]  eta: 0:18:31  lr: 0.000061  loss: 0.9196 (1.1953)  time: 0.9267  data: 0.0035  max mem: 55008
[00:28:39.745914] Epoch: [0]  [ 180/1251]  eta: 0:18:00  lr: 0.000069  loss: 0.8981 (1.1621)  time: 0.9292  data: 0.0006  max mem: 55008
[00:28:58.201332] Epoch: [0]  [ 200/1251]  eta: 0:17:30  lr: 0.000077  loss: 0.8554 (1.1314)  time: 0.9227  data: 0.0007  max mem: 55008
[00:29:16.818314] Epoch: [0]  [ 220/1251]  eta: 0:17:04  lr: 0.000084  loss: 0.8344 (1.1048)  time: 0.9308  data: 0.0032  max mem: 55008
[00:29:35.418235] Epoch: [0]  [ 240/1251]  eta: 0:16:39  lr: 0.000092  loss: 0.8108 (1.0809)  time: 0.9299  data: 0.0007  max mem: 55008
[00:29:53.987808] Epoch: [0]  [ 260/1251]  eta: 0:16:14  lr: 0.000100  loss: 0.8054 (1.0599)  time: 0.9284  data: 0.0005  max mem: 55008
[00:30:12.473924] Epoch: [0]  [ 280/1251]  eta: 0:15:51  lr: 0.000107  loss: 0.7911 (1.0410)  time: 0.9242  data: 0.0007  max mem: 55008
[00:30:31.008277] Epoch: [0]  [ 300/1251]  eta: 0:15:28  lr: 0.000115  loss: 0.7800 (1.0240)  time: 0.9267  data: 0.0033  max mem: 55008
[00:30:49.466975] Epoch: [0]  [ 320/1251]  eta: 0:15:05  lr: 0.000123  loss: 0.7809 (1.0089)  time: 0.9229  data: 0.0006  max mem: 55008
[00:31:08.412888] Epoch: [0]  [ 340/1251]  eta: 0:14:44  lr: 0.000130  loss: 0.7766 (0.9953)  time: 0.9472  data: 0.0006  max mem: 55008
[00:31:26.828356] Epoch: [0]  [ 360/1251]  eta: 0:14:22  lr: 0.000138  loss: 0.7627 (0.9824)  time: 0.9207  data: 0.0006  max mem: 55008
[00:31:45.465631] Epoch: [0]  [ 380/1251]  eta: 0:14:01  lr: 0.000146  loss: 0.7632 (0.9708)  time: 0.9318  data: 0.0006  max mem: 55008
[00:32:04.100517] Epoch: [0]  [ 400/1251]  eta: 0:13:41  lr: 0.000153  loss: 0.7549 (0.9600)  time: 0.9317  data: 0.0007  max mem: 55008
[00:32:22.937315] Epoch: [0]  [ 420/1251]  eta: 0:13:20  lr: 0.000161  loss: 0.7424 (0.9500)  time: 0.9418  data: 0.0006  max mem: 55008
[00:32:41.539773] Epoch: [0]  [ 440/1251]  eta: 0:13:00  lr: 0.000169  loss: 0.7436 (0.9407)  time: 0.9295  data: 0.0024  max mem: 55008
[00:33:00.172138] Epoch: [0]  [ 460/1251]  eta: 0:12:40  lr: 0.000176  loss: 0.7784 (0.9337)  time: 0.9316  data: 0.0041  max mem: 55008
[00:33:18.684520] Epoch: [0]  [ 480/1251]  eta: 0:12:19  lr: 0.000184  loss: 0.7499 (0.9262)  time: 0.9256  data: 0.0036  max mem: 55008
[00:33:37.308641] Epoch: [0]  [ 500/1251]  eta: 0:11:59  lr: 0.000192  loss: 0.7521 (0.9193)  time: 0.9312  data: 0.0006  max mem: 55008
[00:33:55.932149] Epoch: [0]  [ 520/1251]  eta: 0:11:39  lr: 0.000200  loss: 0.7443 (0.9126)  time: 0.9311  data: 0.0006  max mem: 55008
[00:34:14.518091] Epoch: [0]  [ 540/1251]  eta: 0:11:19  lr: 0.000207  loss: 0.7374 (0.9062)  time: 0.9292  data: 0.0006  max mem: 55008
[00:34:33.069338] Epoch: [0]  [ 560/1251]  eta: 0:10:59  lr: 0.000215  loss: 0.7299 (0.8998)  time: 0.9275  data: 0.0040  max mem: 55008
[00:34:51.716944] Epoch: [0]  [ 580/1251]  eta: 0:10:40  lr: 0.000223  loss: 0.7375 (0.8941)  time: 0.9323  data: 0.0042  max mem: 55008
[00:35:10.502241] Epoch: [0]  [ 600/1251]  eta: 0:10:20  lr: 0.000230  loss: 0.7459 (0.8892)  time: 0.9392  data: 0.0007  max mem: 55008
[00:35:29.103010] Epoch: [0]  [ 620/1251]  eta: 0:10:01  lr: 0.000238  loss: 0.7323 (0.8842)  time: 0.9300  data: 0.0008  max mem: 55008
[00:35:47.585660] Epoch: [0]  [ 640/1251]  eta: 0:09:41  lr: 0.000246  loss: 0.7298 (0.8794)  time: 0.9240  data: 0.0006  max mem: 55008
[00:36:06.378475] Epoch: [0]  [ 660/1251]  eta: 0:09:22  lr: 0.000253  loss: 0.7160 (0.8746)  time: 0.9391  data: 0.0006  max mem: 55008
[00:36:25.016838] Epoch: [0]  [ 680/1251]  eta: 0:09:03  lr: 0.000261  loss: 0.7139 (0.8698)  time: 0.9319  data: 0.0006  max mem: 55008
[00:36:43.591551] Epoch: [0]  [ 700/1251]  eta: 0:08:43  lr: 0.000269  loss: 0.7065 (0.8652)  time: 0.9287  data: 0.0006  max mem: 55008
[00:37:02.227748] Epoch: [0]  [ 720/1251]  eta: 0:08:24  lr: 0.000276  loss: 0.7114 (0.8610)  time: 0.9317  data: 0.0006  max mem: 55008
[00:37:20.619577] Epoch: [0]  [ 740/1251]  eta: 0:08:05  lr: 0.000284  loss: 0.7004 (0.8569)  time: 0.9195  data: 0.0006  max mem: 55008
[00:37:39.348332] Epoch: [0]  [ 760/1251]  eta: 0:07:45  lr: 0.000292  loss: 0.6880 (0.8526)  time: 0.9364  data: 0.0005  max mem: 55008
[00:37:57.953050] Epoch: [0]  [ 780/1251]  eta: 0:07:26  lr: 0.000299  loss: 0.6832 (0.8484)  time: 0.9302  data: 0.0036  max mem: 55008
[00:38:16.529851] Epoch: [0]  [ 800/1251]  eta: 0:07:07  lr: 0.000307  loss: 0.7038 (0.8447)  time: 0.9288  data: 0.0009  max mem: 55008
[00:38:35.222570] Epoch: [0]  [ 820/1251]  eta: 0:06:48  lr: 0.000315  loss: 0.6908 (0.8410)  time: 0.9346  data: 0.0040  max mem: 55008
[00:38:53.954044] Epoch: [0]  [ 840/1251]  eta: 0:06:29  lr: 0.000322  loss: 0.6854 (0.8372)  time: 0.9365  data: 0.0063  max mem: 55008
[00:39:12.522029] Epoch: [0]  [ 860/1251]  eta: 0:06:10  lr: 0.000330  loss: 0.6715 (0.8333)  time: 0.9283  data: 0.0033  max mem: 55008
[00:39:31.225749] Epoch: [0]  [ 880/1251]  eta: 0:05:51  lr: 0.000338  loss: 0.6728 (0.8297)  time: 0.9351  data: 0.0006  max mem: 55008
[00:39:49.760225] Epoch: [0]  [ 900/1251]  eta: 0:05:32  lr: 0.000345  loss: 0.6784 (0.8263)  time: 0.9267  data: 0.0038  max mem: 55008
[00:40:08.442485] Epoch: [0]  [ 920/1251]  eta: 0:05:13  lr: 0.000353  loss: 0.6738 (0.8230)  time: 0.9341  data: 0.0006  max mem: 55008
[00:40:26.888428] Epoch: [0]  [ 940/1251]  eta: 0:04:54  lr: 0.000361  loss: 0.6692 (0.8198)  time: 0.9222  data: 0.0007  max mem: 55008
[00:40:45.418625] Epoch: [0]  [ 960/1251]  eta: 0:04:34  lr: 0.000368  loss: 0.6582 (0.8164)  time: 0.9265  data: 0.0038  max mem: 55008
[00:41:04.037414] Epoch: [0]  [ 980/1251]  eta: 0:04:16  lr: 0.000376  loss: 0.6560 (0.8131)  time: 0.9308  data: 0.0006  max mem: 55008
[00:41:22.452506] Epoch: [0]  [1000/1251]  eta: 0:03:57  lr: 0.000384  loss: 0.6536 (0.8100)  time: 0.9207  data: 0.0006  max mem: 55008
[00:41:40.976248] Epoch: [0]  [1020/1251]  eta: 0:03:38  lr: 0.000391  loss: 0.6534 (0.8070)  time: 0.9261  data: 0.0006  max mem: 55008
[00:41:59.477497] Epoch: [0]  [1040/1251]  eta: 0:03:19  lr: 0.000399  loss: 0.6623 (0.8043)  time: 0.9250  data: 0.0005  max mem: 55008
[00:42:17.710723] Epoch: [0]  [1060/1251]  eta: 0:03:00  lr: 0.000407  loss: 0.6607 (0.8015)  time: 0.9116  data: 0.0006  max mem: 55008
[00:42:36.319317] Epoch: [0]  [1080/1251]  eta: 0:02:41  lr: 0.000414  loss: 0.6557 (0.7988)  time: 0.9304  data: 0.0005  max mem: 55008
[00:42:54.789521] Epoch: [0]  [1100/1251]  eta: 0:02:22  lr: 0.000422  loss: 0.6464 (0.7961)  time: 0.9235  data: 0.0005  max mem: 55008
[00:43:13.321847] Epoch: [0]  [1120/1251]  eta: 0:02:03  lr: 0.000430  loss: 0.6565 (0.7936)  time: 0.9266  data: 0.0037  max mem: 55008
[00:43:31.781181] Epoch: [0]  [1140/1251]  eta: 0:01:44  lr: 0.000437  loss: 0.6637 (0.7913)  time: 0.9229  data: 0.0005  max mem: 55008
[00:43:50.409845] Epoch: [0]  [1160/1251]  eta: 0:01:25  lr: 0.000445  loss: 0.6565 (0.7889)  time: 0.9314  data: 0.0005  max mem: 55008
[00:44:08.828881] Epoch: [0]  [1180/1251]  eta: 0:01:06  lr: 0.000453  loss: 0.6465 (0.7865)  time: 0.9209  data: 0.0005  max mem: 55008
[00:44:27.390549] Epoch: [0]  [1200/1251]  eta: 0:00:47  lr: 0.000460  loss: 0.6440 (0.7842)  time: 0.9280  data: 0.0005  max mem: 55008
[00:44:45.803950] Epoch: [0]  [1220/1251]  eta: 0:00:29  lr: 0.000468  loss: 0.6499 (0.7819)  time: 0.9206  data: 0.0005  max mem: 55008
[00:45:03.873144] Epoch: [0]  [1240/1251]  eta: 0:00:10  lr: 0.000476  loss: 0.6469 (0.7798)  time: 0.9034  data: 0.0007  max mem: 55008
[00:45:12.405634] Epoch: [0]  [1250/1251]  eta: 0:00:00  lr: 0.000479  loss: 0.6469 (0.7787)  time: 0.8657  data: 0.0005  max mem: 55008
[00:45:13.206456] Epoch: [0] Total time: 0:19:36 (0.9401 s / it)
[00:45:13.234762] Averaged stats: lr: 0.000479  loss: 0.6469 (0.7787)
[00:45:14.274108] log_dir: /ibex/ai/project/c2090/lomar_plus_save/logs/raven/mae_encoderonly_mask_0.8_neigh_5_wind_9_num_4_epoches_100_r_384
[00:45:22.116268] Epoch: [1]  [   0/1251]  eta: 2:43:29  lr: 0.000480  loss: 0.6180 (0.6180)  time: 7.8410  data: 6.8421  max mem: 55008
[00:45:41.979722] Epoch: [1]  [  20/1251]  eta: 0:27:03  lr: 0.000488  loss: 0.6371 (0.6348)  time: 0.9931  data: 0.0043  max mem: 55008
[00:46:00.768769] Epoch: [1]  [  40/1251]  eta: 0:22:53  lr: 0.000495  loss: 0.6355 (0.6344)  time: 0.9394  data: 0.0005  max mem: 55008
[00:46:19.517817] Epoch: [1]  [  60/1251]  eta: 0:21:13  lr: 0.000503  loss: 0.6348 (0.6352)  time: 0.9374  data: 0.0003  max mem: 55008
[00:46:38.245195] Epoch: [1]  [  80/1251]  eta: 0:20:13  lr: 0.000511  loss: 0.6318 (0.6346)  time: 0.9363  data: 0.0004  max mem: 55008
[00:46:56.971128] Epoch: [1]  [ 100/1251]  eta: 0:19:30  lr: 0.000518  loss: 0.6418 (0.6352)  time: 0.9362  data: 0.0005  max mem: 55008
[00:47:15.776855] Epoch: [1]  [ 120/1251]  eta: 0:18:55  lr: 0.000526  loss: 0.6383 (0.6351)  time: 0.9402  data: 0.0004  max mem: 55008
[00:47:34.478656] Epoch: [1]  [ 140/1251]  eta: 0:18:24  lr: 0.000534  loss: 0.6238 (0.6332)  time: 0.9350  data: 0.0009  max mem: 55008
[00:47:53.202681] Epoch: [1]  [ 160/1251]  eta: 0:17:56  lr: 0.000541  loss: 0.6210 (0.6318)  time: 0.9362  data: 0.0005  max mem: 55008
[00:48:11.981955] Epoch: [1]  [ 180/1251]  eta: 0:17:31  lr: 0.000549  loss: 0.6184 (0.6305)  time: 0.9389  data: 0.0040  max mem: 55008
[00:48:30.817359] Epoch: [1]  [ 200/1251]  eta: 0:17:07  lr: 0.000557  loss: 0.6143 (0.6294)  time: 0.9417  data: 0.0005  max mem: 55008
[00:48:49.704144] Epoch: [1]  [ 220/1251]  eta: 0:16:44  lr: 0.000564  loss: 0.6281 (0.6289)  time: 0.9443  data: 0.0039  max mem: 55008
[00:49:08.591746] Epoch: [1]  [ 240/1251]  eta: 0:16:22  lr: 0.000572  loss: 0.6219 (0.6285)  time: 0.9443  data: 0.0004  max mem: 55008
[00:49:27.502683] Epoch: [1]  [ 260/1251]  eta: 0:16:01  lr: 0.000580  loss: 0.6250 (0.6281)  time: 0.9455  data: 0.0035  max mem: 55008
[00:49:46.209769] Epoch: [1]  [ 280/1251]  eta: 0:15:39  lr: 0.000587  loss: 0.6130 (0.6271)  time: 0.9353  data: 0.0004  max mem: 55008
[00:50:05.048208] Epoch: [1]  [ 300/1251]  eta: 0:15:18  lr: 0.000595  loss: 0.6121 (0.6260)  time: 0.9419  data: 0.0036  max mem: 55008
[00:50:23.865956] Epoch: [1]  [ 320/1251]  eta: 0:14:57  lr: 0.000603  loss: 0.5999 (0.6246)  time: 0.9408  data: 0.0035  max mem: 55008
[00:50:42.734772] Epoch: [1]  [ 340/1251]  eta: 0:14:37  lr: 0.000610  loss: 0.6084 (0.6234)  time: 0.9434  data: 0.0037  max mem: 55008
[00:51:01.583011] Epoch: [1]  [ 360/1251]  eta: 0:14:17  lr: 0.000618  loss: 0.6036 (0.6223)  time: 0.9424  data: 0.0004  max mem: 55008
[00:51:20.309545] Epoch: [1]  [ 380/1251]  eta: 0:13:56  lr: 0.000626  loss: 0.5927 (0.6209)  time: 0.9363  data: 0.0004  max mem: 55008
[00:51:38.968946] Epoch: [1]  [ 400/1251]  eta: 0:13:36  lr: 0.000633  loss: 0.6021 (0.6197)  time: 0.9329  data: 0.0033  max mem: 55008
[00:51:57.948304] Epoch: [1]  [ 420/1251]  eta: 0:13:16  lr: 0.000641  loss: 0.6144 (0.6197)  time: 0.9461  data: 0.0008  max mem: 55008
[00:52:16.434912] Epoch: [1]  [ 440/1251]  eta: 0:12:56  lr: 0.000649  loss: 0.6065 (0.6191)  time: 0.9243  data: 0.0005  max mem: 55008
[00:52:35.349350] Epoch: [1]  [ 460/1251]  eta: 0:12:36  lr: 0.000656  loss: 0.6016 (0.6185)  time: 0.9457  data: 0.0007  max mem: 55008
[00:52:54.192697] Epoch: [1]  [ 480/1251]  eta: 0:12:17  lr: 0.000664  loss: 0.6054 (0.6179)  time: 0.9421  data: 0.0004  max mem: 55008
[00:53:12.842930] Epoch: [1]  [ 500/1251]  eta: 0:11:57  lr: 0.000672  loss: 0.5984 (0.6169)  time: 0.9325  data: 0.0005  max mem: 55008
[00:53:31.541889] Epoch: [1]  [ 520/1251]  eta: 0:11:37  lr: 0.000680  loss: 0.5829 (0.6157)  time: 0.9349  data: 0.0005  max mem: 55008
[00:53:50.239975] Epoch: [1]  [ 540/1251]  eta: 0:11:17  lr: 0.000687  loss: 0.5809 (0.6144)  time: 0.9348  data: 0.0006  max mem: 55008
[00:54:09.021937] Epoch: [1]  [ 560/1251]  eta: 0:10:58  lr: 0.000695  loss: 0.6075 (0.6141)  time: 0.9390  data: 0.0032  max mem: 55008
[00:54:27.898330] Epoch: [1]  [ 580/1251]  eta: 0:10:39  lr: 0.000703  loss: 0.5940 (0.6135)  time: 0.9438  data: 0.0004  max mem: 55008
[00:54:46.779852] Epoch: [1]  [ 600/1251]  eta: 0:10:20  lr: 0.000710  loss: 0.6027 (0.6132)  time: 0.9440  data: 0.0004  max mem: 55008
[00:55:05.410243] Epoch: [1]  [ 620/1251]  eta: 0:10:00  lr: 0.000718  loss: 0.5901 (0.6125)  time: 0.9315  data: 0.0004  max mem: 55008
[00:55:24.232888] Epoch: [1]  [ 640/1251]  eta: 0:09:41  lr: 0.000726  loss: 0.5850 (0.6117)  time: 0.9411  data: 0.0003  max mem: 55008
[00:55:42.896623] Epoch: [1]  [ 660/1251]  eta: 0:09:21  lr: 0.000733  loss: 0.5861 (0.6109)  time: 0.9331  data: 0.0035  max mem: 55008
[00:56:01.468732] Epoch: [1]  [ 680/1251]  eta: 0:09:02  lr: 0.000741  loss: 0.5746 (0.6099)  time: 0.9286  data: 0.0004  max mem: 55008
[00:56:20.086539] Epoch: [1]  [ 700/1251]  eta: 0:08:43  lr: 0.000749  loss: 0.5779 (0.6090)  time: 0.9308  data: 0.0030  max mem: 55008
[00:56:38.940432] Epoch: [1]  [ 720/1251]  eta: 0:08:24  lr: 0.000756  loss: 0.5820 (0.6082)  time: 0.9426  data: 0.0005  max mem: 55008
[00:56:57.651638] Epoch: [1]  [ 740/1251]  eta: 0:08:04  lr: 0.000764  loss: 0.5726 (0.6073)  time: 0.9355  data: 0.0004  max mem: 55008
[00:57:16.397381] Epoch: [1]  [ 760/1251]  eta: 0:07:45  lr: 0.000772  loss: 0.5730 (0.6065)  time: 0.9372  data: 0.0004  max mem: 55008
[00:57:35.034771] Epoch: [1]  [ 780/1251]  eta: 0:07:26  lr: 0.000779  loss: 0.6177 (0.6068)  time: 0.9318  data: 0.0005  max mem: 55008
[00:57:53.646568] Epoch: [1]  [ 800/1251]  eta: 0:07:07  lr: 0.000787  loss: 0.6098 (0.6070)  time: 0.9305  data: 0.0010  max mem: 55008
[00:58:12.531863] Epoch: [1]  [ 820/1251]  eta: 0:06:48  lr: 0.000795  loss: 0.5908 (0.6066)  time: 0.9442  data: 0.0032  max mem: 55008
[00:58:31.404453] Epoch: [1]  [ 840/1251]  eta: 0:06:29  lr: 0.000802  loss: 0.5787 (0.6060)  time: 0.9436  data: 0.0040  max mem: 55008
[00:58:50.174511] Epoch: [1]  [ 860/1251]  eta: 0:06:10  lr: 0.000810  loss: 0.5802 (0.6056)  time: 0.9385  data: 0.0004  max mem: 55008
[00:59:08.859918] Epoch: [1]  [ 880/1251]  eta: 0:05:51  lr: 0.000818  loss: 0.5760 (0.6050)  time: 0.9342  data: 0.0009  max mem: 55008
[00:59:27.568600] Epoch: [1]  [ 900/1251]  eta: 0:05:32  lr: 0.000825  loss: 0.5826 (0.6045)  time: 0.9354  data: 0.0004  max mem: 55008
[00:59:46.302470] Epoch: [1]  [ 920/1251]  eta: 0:05:13  lr: 0.000833  loss: 0.5744 (0.6039)  time: 0.9366  data: 0.0005  max mem: 55008
[01:00:05.092227] Epoch: [1]  [ 940/1251]  eta: 0:04:54  lr: 0.000841  loss: 0.5658 (0.6031)  time: 0.9394  data: 0.0004  max mem: 55008
[01:00:23.792198] Epoch: [1]  [ 960/1251]  eta: 0:04:35  lr: 0.000848  loss: 0.5658 (0.6023)  time: 0.9350  data: 0.0004  max mem: 55008
[01:00:42.581894] Epoch: [1]  [ 980/1251]  eta: 0:04:16  lr: 0.000856  loss: 0.5663 (0.6015)  time: 0.9394  data: 0.0005  max mem: 55008
[01:01:01.423740] Epoch: [1]  [1000/1251]  eta: 0:03:57  lr: 0.000864  loss: 0.5753 (0.6010)  time: 0.9420  data: 0.0004  max mem: 55008
[01:01:20.122940] Epoch: [1]  [1020/1251]  eta: 0:03:38  lr: 0.000871  loss: 0.5580 (0.6003)  time: 0.9349  data: 0.0004  max mem: 55008
[01:01:38.721527] Epoch: [1]  [1040/1251]  eta: 0:03:19  lr: 0.000879  loss: 0.5706 (0.5997)  time: 0.9299  data: 0.0004  max mem: 55008
[01:01:57.454733] Epoch: [1]  [1060/1251]  eta: 0:03:00  lr: 0.000887  loss: 0.5701 (0.5992)  time: 0.9366  data: 0.0005  max mem: 55008
[01:02:16.414008] Epoch: [1]  [1080/1251]  eta: 0:02:41  lr: 0.000894  loss: 0.5662 (0.5987)  time: 0.9479  data: 0.0004  max mem: 55008
[01:02:35.202099] Epoch: [1]  [1100/1251]  eta: 0:02:22  lr: 0.000902  loss: 0.5725 (0.5982)  time: 0.9394  data: 0.0004  max mem: 55008
[01:02:53.933105] Epoch: [1]  [1120/1251]  eta: 0:02:03  lr: 0.000910  loss: 0.5687 (0.5977)  time: 0.9365  data: 0.0004  max mem: 55008
[01:03:12.747830] Epoch: [1]  [1140/1251]  eta: 0:01:44  lr: 0.000917  loss: 0.5639 (0.5971)  time: 0.9407  data: 0.0004  max mem: 55008
[01:03:31.639749] Epoch: [1]  [1160/1251]  eta: 0:01:26  lr: 0.000925  loss: 0.5661 (0.5967)  time: 0.9446  data: 0.0039  max mem: 55008
[01:03:50.440556] Epoch: [1]  [1180/1251]  eta: 0:01:07  lr: 0.000933  loss: 0.5621 (0.5961)  time: 0.9400  data: 0.0004  max mem: 55008
[01:04:08.976439] Epoch: [1]  [1200/1251]  eta: 0:00:48  lr: 0.000940  loss: 0.5588 (0.5956)  time: 0.9267  data: 0.0005  max mem: 55008
[01:04:27.761170] Epoch: [1]  [1220/1251]  eta: 0:00:29  lr: 0.000948  loss: 0.5635 (0.5951)  time: 0.9392  data: 0.0004  max mem: 55008
[01:04:45.969960] Epoch: [1]  [1240/1251]  eta: 0:00:10  lr: 0.000956  loss: 0.5591 (0.5946)  time: 0.9104  data: 0.0007  max mem: 55008
[01:04:54.521471] Epoch: [1]  [1250/1251]  eta: 0:00:00  lr: 0.000959  loss: 0.5577 (0.5943)  time: 0.8670  data: 0.0005  max mem: 55008
[01:04:55.452575] Epoch: [1] Total time: 0:19:41 (0.9442 s / it)
[01:04:55.453409] Averaged stats: lr: 0.000959  loss: 0.5577 (0.5945)
[01:04:55.456635] log_dir: /ibex/ai/project/c2090/lomar_plus_save/logs/raven/mae_encoderonly_mask_0.8_neigh_5_wind_9_num_4_epoches_100_r_384
[01:05:03.274249] Epoch: [2]  [   0/1251]  eta: 2:42:58  lr: 0.000960  loss: 0.5288 (0.5288)  time: 7.8166  data: 6.7242  max mem: 55008
[01:05:23.892277] Epoch: [2]  [  20/1251]  eta: 0:27:46  lr: 0.000968  loss: 0.5652 (0.5654)  time: 1.0308  data: 0.0223  max mem: 55008
[01:05:42.589288] Epoch: [2]  [  40/1251]  eta: 0:23:12  lr: 0.000975  loss: 0.5696 (0.5664)  time: 0.9348  data: 0.0005  max mem: 55008
[01:06:01.261765] Epoch: [2]  [  60/1251]  eta: 0:21:24  lr: 0.000983  loss: 0.5588 (0.5652)  time: 0.9336  data: 0.0042  max mem: 55008
[01:06:20.128765] Epoch: [2]  [  80/1251]  eta: 0:20:24  lr: 0.000991  loss: 0.5608 (0.5635)  time: 0.9433  data: 0.0035  max mem: 55008
[01:06:39.114736] Epoch: [2]  [ 100/1251]  eta: 0:19:41  lr: 0.000998  loss: 0.5486 (0.5611)  time: 0.9493  data: 0.0005  max mem: 55008
[01:06:57.914556] Epoch: [2]  [ 120/1251]  eta: 0:19:04  lr: 0.001006  loss: 0.5514 (0.5597)  time: 0.9399  data: 0.0004  max mem: 55008
[01:07:16.874450] Epoch: [2]  [ 140/1251]  eta: 0:18:34  lr: 0.001014  loss: 0.5643 (0.5603)  time: 0.9479  data: 0.0004  max mem: 55008
[01:07:35.751976] Epoch: [2]  [ 160/1251]  eta: 0:18:06  lr: 0.001021  loss: 0.5561 (0.5595)  time: 0.9438  data: 0.0041  max mem: 55008
[01:07:54.709175] Epoch: [2]  [ 180/1251]  eta: 0:17:40  lr: 0.001029  loss: 0.5524 (0.5590)  time: 0.9478  data: 0.0072  max mem: 55008
[01:08:13.589606] Epoch: [2]  [ 200/1251]  eta: 0:17:15  lr: 0.001037  loss: 0.5520 (0.5582)  time: 0.9440  data: 0.0005  max mem: 55008
[01:08:32.639382] Epoch: [2]  [ 220/1251]  eta: 0:16:53  lr: 0.001044  loss: 0.5605 (0.5584)  time: 0.9524  data: 0.0005  max mem: 55008
[01:08:51.521901] Epoch: [2]  [ 240/1251]  eta: 0:16:29  lr: 0.001052  loss: 0.5488 (0.5582)  time: 0.9404  data: 0.0005  max mem: 55008
[01:09:10.259797] Epoch: [2]  [ 260/1251]  eta: 0:16:07  lr: 0.001060  loss: 0.5470 (0.5572)  time: 0.9368  data: 0.0004  max mem: 55008
[01:09:29.090877] Epoch: [2]  [ 280/1251]  eta: 0:15:45  lr: 0.001067  loss: 0.5530 (0.5567)  time: 0.9415  data: 0.0005  max mem: 55008
[01:09:47.798280] Epoch: [2]  [ 300/1251]  eta: 0:15:23  lr: 0.001075  loss: 0.5584 (0.5568)  time: 0.9353  data: 0.0004  max mem: 55008
[01:10:06.614695] Epoch: [2]  [ 320/1251]  eta: 0:15:02  lr: 0.001083  loss: 0.5631 (0.5572)  time: 0.9408  data: 0.0005  max mem: 55008
[01:10:25.267724] Epoch: [2]  [ 340/1251]  eta: 0:14:40  lr: 0.001090  loss: 0.5550 (0.5570)  time: 0.9326  data: 0.0005  max mem: 55008
[01:10:44.234203] Epoch: [2]  [ 360/1251]  eta: 0:14:20  lr: 0.001098  loss: 0.5623 (0.5573)  time: 0.9483  data: 0.0006  max mem: 55008
[01:11:02.999680] Epoch: [2]  [ 380/1251]  eta: 0:14:00  lr: 0.001106  loss: 0.5830 (0.5584)  time: 0.9382  data: 0.0007  max mem: 55008
[01:11:21.552102] Epoch: [2]  [ 400/1251]  eta: 0:13:39  lr: 0.001113  loss: 0.5540 (0.5583)  time: 0.9276  data: 0.0033  max mem: 55008
[01:11:40.226036] Epoch: [2]  [ 420/1251]  eta: 0:13:18  lr: 0.001121  loss: 0.5646 (0.5585)  time: 0.9337  data: 0.0004  max mem: 55008
[01:11:59.168872] Epoch: [2]  [ 440/1251]  eta: 0:12:59  lr: 0.001129  loss: 0.5499 (0.5582)  time: 0.9471  data: 0.0004  max mem: 55008
[01:12:17.992961] Epoch: [2]  [ 460/1251]  eta: 0:12:39  lr: 0.001136  loss: 0.5415 (0.5577)  time: 0.9411  data: 0.0017  max mem: 55008
[01:12:36.766674] Epoch: [2]  [ 480/1251]  eta: 0:12:19  lr: 0.001144  loss: 0.5416 (0.5571)  time: 0.9386  data: 0.0004  max mem: 55008
[01:12:55.400570] Epoch: [2]  [ 500/1251]  eta: 0:11:59  lr: 0.001152  loss: 0.5418 (0.5566)  time: 0.9316  data: 0.0004  max mem: 55008
[01:13:14.306918] Epoch: [2]  [ 520/1251]  eta: 0:11:39  lr: 0.001160  loss: 0.5354 (0.5558)  time: 0.9453  data: 0.0005  max mem: 55008
[01:13:33.182349] Epoch: [2]  [ 540/1251]  eta: 0:11:20  lr: 0.001167  loss: 0.5497 (0.5555)  time: 0.9437  data: 0.0005  max mem: 55008
[01:13:51.873464] Epoch: [2]  [ 560/1251]  eta: 0:11:00  lr: 0.001175  loss: 0.5561 (0.5555)  time: 0.9345  data: 0.0004  max mem: 55008
[01:14:10.523925] Epoch: [2]  [ 580/1251]  eta: 0:10:40  lr: 0.001183  loss: 0.5596 (0.5557)  time: 0.9325  data: 0.0034  max mem: 55008
[01:14:29.116799] Epoch: [2]  [ 600/1251]  eta: 0:10:21  lr: 0.001190  loss: 0.5550 (0.5558)  time: 0.9296  data: 0.0029  max mem: 55008
[01:14:47.992614] Epoch: [2]  [ 620/1251]  eta: 0:10:01  lr: 0.001198  loss: 0.5348 (0.5554)  time: 0.9437  data: 0.0003  max mem: 55008
[01:15:06.819634] Epoch: [2]  [ 640/1251]  eta: 0:09:42  lr: 0.001206  loss: 0.5422 (0.5551)  time: 0.9413  data: 0.0005  max mem: 55008
[01:15:25.585039] Epoch: [2]  [ 660/1251]  eta: 0:09:23  lr: 0.001213  loss: 0.5492 (0.5548)  time: 0.9382  data: 0.0006  max mem: 55008
[01:15:44.085542] Epoch: [2]  [ 680/1251]  eta: 0:09:03  lr: 0.001221  loss: 0.5411 (0.5545)  time: 0.9250  data: 0.0004  max mem: 55008
[01:16:02.753197] Epoch: [2]  [ 700/1251]  eta: 0:08:44  lr: 0.001229  loss: 0.5520 (0.5545)  time: 0.9333  data: 0.0004  max mem: 55008
[01:16:21.532203] Epoch: [2]  [ 720/1251]  eta: 0:08:25  lr: 0.001236  loss: 0.5482 (0.5543)  time: 0.9389  data: 0.0005  max mem: 55008
[01:16:40.354066] Epoch: [2]  [ 740/1251]  eta: 0:08:06  lr: 0.001244  loss: 0.5494 (0.5541)  time: 0.9410  data: 0.0006  max mem: 55008
[01:16:59.246237] Epoch: [2]  [ 760/1251]  eta: 0:07:46  lr: 0.001252  loss: 0.5444 (0.5538)  time: 0.9446  data: 0.0005  max mem: 55008
[01:17:17.909886] Epoch: [2]  [ 780/1251]  eta: 0:07:27  lr: 0.001259  loss: 0.5453 (0.5536)  time: 0.9331  data: 0.0004  max mem: 55008
[01:17:36.694826] Epoch: [2]  [ 800/1251]  eta: 0:07:08  lr: 0.001267  loss: 0.5409 (0.5534)  time: 0.9353  data: 0.0003  max mem: 55008
[01:17:55.517171] Epoch: [2]  [ 820/1251]  eta: 0:06:49  lr: 0.001275  loss: 0.5436 (0.5531)  time: 0.9411  data: 0.0011  max mem: 55008
[01:18:14.354662] Epoch: [2]  [ 840/1251]  eta: 0:06:30  lr: 0.001282  loss: 0.5468 (0.5529)  time: 0.9418  data: 0.0005  max mem: 55008
[01:18:33.149015] Epoch: [2]  [ 860/1251]  eta: 0:06:11  lr: 0.001290  loss: 0.5424 (0.5527)  time: 0.9397  data: 0.0042  max mem: 55008
[01:18:51.883305] Epoch: [2]  [ 880/1251]  eta: 0:05:52  lr: 0.001298  loss: 0.5555 (0.5529)  time: 0.9367  data: 0.0004  max mem: 55008
[01:19:10.706783] Epoch: [2]  [ 900/1251]  eta: 0:05:33  lr: 0.001305  loss: 0.5830 (0.5536)  time: 0.9411  data: 0.0004  max mem: 55008
[01:19:29.401745] Epoch: [2]  [ 920/1251]  eta: 0:05:14  lr: 0.001313  loss: 0.5980 (0.5547)  time: 0.9347  data: 0.0004  max mem: 55008
[01:19:48.279462] Epoch: [2]  [ 940/1251]  eta: 0:04:55  lr: 0.001321  loss: 0.5997 (0.5555)  time: 0.9438  data: 0.0008  max mem: 55008
[01:20:06.973040] Epoch: [2]  [ 960/1251]  eta: 0:04:35  lr: 0.001328  loss: 0.5812 (0.5561)  time: 0.9346  data: 0.0005  max mem: 55008
[01:20:25.887859] Epoch: [2]  [ 980/1251]  eta: 0:04:16  lr: 0.001336  loss: 0.5598 (0.5561)  time: 0.9457  data: 0.0004  max mem: 55008
[01:20:44.698078] Epoch: [2]  [1000/1251]  eta: 0:03:57  lr: 0.001344  loss: 0.5470 (0.5559)  time: 0.9405  data: 0.0004  max mem: 55008
[01:21:03.513768] Epoch: [2]  [1020/1251]  eta: 0:03:38  lr: 0.001351  loss: 0.5492 (0.5557)  time: 0.9407  data: 0.0004  max mem: 55008
[01:21:22.417109] Epoch: [2]  [1040/1251]  eta: 0:03:20  lr: 0.001359  loss: 0.5317 (0.5553)  time: 0.9451  data: 0.0004  max mem: 55008
[01:21:41.261946] Epoch: [2]  [1060/1251]  eta: 0:03:01  lr: 0.001367  loss: 0.5306 (0.5548)  time: 0.9422  data: 0.0004  max mem: 55008
[01:22:00.147399] Epoch: [2]  [1080/1251]  eta: 0:02:42  lr: 0.001374  loss: 0.5432 (0.5545)  time: 0.9442  data: 0.0041  max mem: 55008
[01:22:19.148921] Epoch: [2]  [1100/1251]  eta: 0:02:23  lr: 0.001382  loss: 0.5365 (0.5542)  time: 0.9500  data: 0.0060  max mem: 55008
[01:22:37.959929] Epoch: [2]  [1120/1251]  eta: 0:02:04  lr: 0.001390  loss: 0.5405 (0.5539)  time: 0.9405  data: 0.0037  max mem: 55008
[01:22:56.779122] Epoch: [2]  [1140/1251]  eta: 0:01:45  lr: 0.001397  loss: 0.5303 (0.5536)  time: 0.9409  data: 0.0032  max mem: 55008
[01:23:15.683277] Epoch: [2]  [1160/1251]  eta: 0:01:26  lr: 0.001405  loss: 0.5264 (0.5532)  time: 0.9451  data: 0.0040  max mem: 55008
[01:23:34.691504] Epoch: [2]  [1180/1251]  eta: 0:01:07  lr: 0.001413  loss: 0.5514 (0.5531)  time: 0.9503  data: 0.0005  max mem: 55008
[01:23:53.600229] Epoch: [2]  [1200/1251]  eta: 0:00:48  lr: 0.001420  loss: 0.5368 (0.5528)  time: 0.9454  data: 0.0005  max mem: 55008
[01:24:12.438850] Epoch: [2]  [1220/1251]  eta: 0:00:29  lr: 0.001428  loss: 0.5364 (0.5525)  time: 0.9418  data: 0.0044  max mem: 55008
[01:24:30.740897] Epoch: [2]  [1240/1251]  eta: 0:00:10  lr: 0.001436  loss: 0.5223 (0.5521)  time: 0.9150  data: 0.0008  max mem: 55008
[01:24:39.279747] Epoch: [2]  [1250/1251]  eta: 0:00:00  lr: 0.001439  loss: 0.5303 (0.5520)  time: 0.8661  data: 0.0006  max mem: 55008
[01:24:40.128372] Epoch: [2] Total time: 0:19:44 (0.9470 s / it)
[01:24:40.277823] Averaged stats: lr: 0.001439  loss: 0.5303 (0.5519)
[01:24:40.282976] log_dir: /ibex/ai/project/c2090/lomar_plus_save/logs/raven/mae_encoderonly_mask_0.8_neigh_5_wind_9_num_4_epoches_100_r_384
[01:24:47.552337] Epoch: [3]  [   0/1251]  eta: 2:31:32  lr: 0.001440  loss: 0.5403 (0.5403)  time: 7.2685  data: 4.9969  max mem: 55008
[01:25:08.741517] Epoch: [3]  [  20/1251]  eta: 0:27:48  lr: 0.001448  loss: 0.5365 (0.5374)  time: 1.0594  data: 0.0429  max mem: 55008
[01:25:27.389286] Epoch: [3]  [  40/1251]  eta: 0:23:11  lr: 0.001455  loss: 0.5437 (0.5402)  time: 0.9323  data: 0.0039  max mem: 55008
[01:25:46.249958] Epoch: [3]  [  60/1251]  eta: 0:21:27  lr: 0.001463  loss: 0.5327 (0.5378)  time: 0.9430  data: 0.0005  max mem: 55008
[01:26:05.048692] Epoch: [3]  [  80/1251]  eta: 0:20:25  lr: 0.001471  loss: 0.5426 (0.5376)  time: 0.9398  data: 0.0004  max mem: 55008
[01:26:23.924525] Epoch: [3]  [ 100/1251]  eta: 0:19:40  lr: 0.001478  loss: 0.5578 (0.5415)  time: 0.9402  data: 0.0031  max mem: 55008
[01:26:42.846905] Epoch: [3]  [ 120/1251]  eta: 0:19:04  lr: 0.001486  loss: 0.5465 (0.5422)  time: 0.9459  data: 0.0005  max mem: 55008
[01:27:01.707506] Epoch: [3]  [ 140/1251]  eta: 0:18:33  lr: 0.001494  loss: 0.5497 (0.5437)  time: 0.9426  data: 0.0004  max mem: 55008
[01:27:20.595171] Epoch: [3]  [ 160/1251]  eta: 0:18:05  lr: 0.001501  loss: 0.5457 (0.5439)  time: 0.9443  data: 0.0037  max mem: 55008
[01:27:39.470399] Epoch: [3]  [ 180/1251]  eta: 0:17:39  lr: 0.001509  loss: 0.5353 (0.5426)  time: 0.9437  data: 0.0040  max mem: 55008
[01:27:58.479854] Epoch: [3]  [ 200/1251]  eta: 0:17:15  lr: 0.001517  loss: 0.5319 (0.5415)  time: 0.9504  data: 0.0005  max mem: 55008
[01:28:17.290987] Epoch: [3]  [ 220/1251]  eta: 0:16:51  lr: 0.001524  loss: 0.5379 (0.5418)  time: 0.9405  data: 0.0036  max mem: 55008
[01:28:36.285415] Epoch: [3]  [ 240/1251]  eta: 0:16:29  lr: 0.001532  loss: 0.5367 (0.5415)  time: 0.9496  data: 0.0075  max mem: 55008
[01:28:55.039960] Epoch: [3]  [ 260/1251]  eta: 0:16:06  lr: 0.001540  loss: 0.5413 (0.5415)  time: 0.9376  data: 0.0004  max mem: 55008
[01:29:14.076329] Epoch: [3]  [ 280/1251]  eta: 0:15:45  lr: 0.001547  loss: 0.5390 (0.5413)  time: 0.9517  data: 0.0035  max mem: 55008
[01:29:33.004758] Epoch: [3]  [ 300/1251]  eta: 0:15:24  lr: 0.001555  loss: 0.5413 (0.5413)  time: 0.9464  data: 0.0005  max mem: 55008
[01:29:51.939922] Epoch: [3]  [ 320/1251]  eta: 0:15:03  lr: 0.001563  loss: 0.5303 (0.5407)  time: 0.9467  data: 0.0005  max mem: 55008
[01:30:10.799478] Epoch: [3]  [ 340/1251]  eta: 0:14:42  lr: 0.001570  loss: 0.5288 (0.5402)  time: 0.9429  data: 0.0004  max mem: 55008
[01:30:29.754672] Epoch: [3]  [ 360/1251]  eta: 0:14:22  lr: 0.001578  loss: 0.5308 (0.5398)  time: 0.9477  data: 0.0040  max mem: 55008
[01:30:48.606679] Epoch: [3]  [ 380/1251]  eta: 0:14:01  lr: 0.001586  loss: 0.5286 (0.5394)  time: 0.9425  data: 0.0007  max mem: 55008
[01:31:07.674124] Epoch: [3]  [ 400/1251]  eta: 0:13:41  lr: 0.001593  loss: 0.5292 (0.5388)  time: 0.9533  data: 0.0004  max mem: 55008
[01:31:26.598934] Epoch: [3]  [ 420/1251]  eta: 0:13:21  lr: 0.001601  loss: 0.5269 (0.5383)  time: 0.9462  data: 0.0074  max mem: 55008
[01:31:45.546756] Epoch: [3]  [ 440/1251]  eta: 0:13:01  lr: 0.001609  loss: 0.5368 (0.5383)  time: 0.9473  data: 0.0003  max mem: 55008
[01:32:04.153277] Epoch: [3]  [ 460/1251]  eta: 0:12:41  lr: 0.001616  loss: 0.5470 (0.5386)  time: 0.9302  data: 0.0004  max mem: 55008
[01:32:23.055709] Epoch: [3]  [ 480/1251]  eta: 0:12:21  lr: 0.001624  loss: 0.5389 (0.5386)  time: 0.9451  data: 0.0042  max mem: 55008
[01:32:41.750132] Epoch: [3]  [ 500/1251]  eta: 0:12:01  lr: 0.001632  loss: 0.5449 (0.5386)  time: 0.9346  data: 0.0005  max mem: 55008
[01:33:00.665351] Epoch: [3]  [ 520/1251]  eta: 0:11:41  lr: 0.001640  loss: 0.5330 (0.5386)  time: 0.9456  data: 0.0006  max mem: 55008
[01:33:19.564917] Epoch: [3]  [ 540/1251]  eta: 0:11:22  lr: 0.001647  loss: 0.5184 (0.5379)  time: 0.9449  data: 0.0038  max mem: 55008
[01:33:38.602556] Epoch: [3]  [ 560/1251]  eta: 0:11:02  lr: 0.001655  loss: 0.5172 (0.5372)  time: 0.9518  data: 0.0035  max mem: 55008
[01:33:57.285030] Epoch: [3]  [ 580/1251]  eta: 0:10:43  lr: 0.001663  loss: 0.5282 (0.5368)  time: 0.9341  data: 0.0004  max mem: 55008
[01:34:16.274237] Epoch: [3]  [ 600/1251]  eta: 0:10:23  lr: 0.001670  loss: 0.5312 (0.5367)  time: 0.9494  data: 0.0005  max mem: 55008
