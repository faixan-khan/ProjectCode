| distributed init (rank 0): env://, gpu 0
| distributed init (rank 2): env://, gpu 2
| distributed init (rank 3): env://, gpu 3
| distributed init (rank 1): env://, gpu 1
[14:05:34.223157] job dir: /home/khanff/cvpr23/lomar
[14:05:34.223563] Namespace(MASTER_ADDR='localhost',
MASTER_PORT='10019',
accum_iter=4,
amp_autocast=True,
batch_size=256,
blr=0.00015,
data_path='/ibex/ai/reference/CV/ILSVR/classification-localization/data/jpeg',
device='cuda',
dist_backend='nccl',
dist_on_itp=False,
dist_url='env://',
distributed=True,
epochs=100,
gpu=0,
input_size=448,
local_rank=0,
log_dir='/ibex/ai/project/c2090/lomar_plus_save/logs/raven/mae_encoderonly_mask_0.8_neigh_0_wind_7_num_4_epoches_100_r_448',
lr=None,
mask_ratio=0.8,
min_lr=0.0,
model='mae_vit_base_patch16_448',
neigh_ratio=0.0,
norm_pix_loss=True,
num_window=4,
num_workers=10,
output_dir='/ibex/ai/project/c2090/lomar_plus_save/checkpoint/raven/mae_encoderonly_mask_0.8_neigh_0_wind_7_num_4_epochs_100_r_448',
pin_mem=True,
rank=0,
resume='',
seed=0,
start_epoch=0,
warmup_epochs=5,
weight_decay=0.05,
window_size=7,
world_size=4)
[14:05:59.122468] Dataset ImageFolder
    Number of datapoints: 1281167
    Root location: /ibex/ai/reference/CV/ILSVR/classification-localization/data/jpeg/train
    StandardTransform
Transform: Compose(
               RandomResizedCrop(size=(448, 448), scale=(0.2, 1.0), ratio=(0.75, 1.3333), interpolation=bicubic)
               RandomHorizontalFlip(p=0.5)
               ToTensor()
               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
           )
[14:05:59.123072] Sampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x2ae39c788b38>
[14:06:00.009107] Model = MaskedAutoencoderViT(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
  )
  (blocks): ModuleList(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (8): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (9): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (10): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (11): Block(
      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=True)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=3072, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (encoder_pred): Linear(in_features=768, out_features=768, bias=True)
)
[14:06:00.009172] base lr: 1.50e-04
[14:06:00.009183] actual lr: 2.40e-03
[14:06:00.009191] accumulate grad iterations: 4
[14:06:00.009199] effective batch size: 4096
[14:06:00.009206] True
[14:06:00.009212] 0
[14:06:00.031708] AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.95)
    eps: 1e-08
    lr: 0.0024
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.95)
    eps: 1e-08
    lr: 0.0024
    weight_decay: 0.05
)
[14:06:00.031778] Start training for 100 epochs
[14:06:00.032321] log_dir: /ibex/ai/project/c2090/lomar_plus_save/logs/raven/mae_encoderonly_mask_0.8_neigh_0_wind_7_num_4_epoches_100_r_448
[14:06:18.994056] Epoch: [0]  [   0/1251]  eta: 6:35:20  lr: 0.000000  loss: 1.8931 (1.8931)  time: 18.9610  data: 12.2055  max mem: 26701
[14:06:31.306703] Epoch: [0]  [  20/1251]  eta: 0:30:33  lr: 0.000008  loss: 1.8690 (1.8375)  time: 0.6155  data: 0.0017  max mem: 27668
[14:06:46.543184] Epoch: [0]  [  40/1251]  eta: 0:22:53  lr: 0.000015  loss: 1.4775 (1.6650)  time: 0.7618  data: 0.0833  max mem: 27668
[14:07:04.035501] Epoch: [0]  [  60/1251]  eta: 0:20:49  lr: 0.000023  loss: 1.2413 (1.5272)  time: 0.8741  data: 0.1887  max mem: 27668
[14:07:17.933203] Epoch: [0]  [  80/1251]  eta: 0:18:45  lr: 0.000031  loss: 1.1072 (1.4252)  time: 0.6948  data: 0.0084  max mem: 27668
[14:07:32.357928] Epoch: [0]  [ 100/1251]  eta: 0:17:31  lr: 0.000038  loss: 1.0514 (1.3509)  time: 0.7212  data: 0.0007  max mem: 27668
[14:07:47.222192] Epoch: [0]  [ 120/1251]  eta: 0:16:41  lr: 0.000046  loss: 1.0060 (1.2942)  time: 0.7425  data: 0.0097  max mem: 27668
[14:08:02.620834] Epoch: [0]  [ 140/1251]  eta: 0:16:05  lr: 0.000054  loss: 0.9834 (1.2502)  time: 0.7698  data: 0.0021  max mem: 27668
[14:08:20.598080] Epoch: [0]  [ 160/1251]  eta: 0:15:52  lr: 0.000061  loss: 0.9548 (1.2140)  time: 0.8988  data: 0.1174  max mem: 27668
[14:08:34.735914] Epoch: [0]  [ 180/1251]  eta: 0:15:15  lr: 0.000069  loss: 0.9111 (1.1806)  time: 0.7068  data: 0.0179  max mem: 27668
[14:08:49.282982] Epoch: [0]  [ 200/1251]  eta: 0:14:44  lr: 0.000077  loss: 0.8665 (1.1493)  time: 0.7273  data: 0.0023  max mem: 27668
[14:09:04.326524] Epoch: [0]  [ 220/1251]  eta: 0:14:19  lr: 0.000084  loss: 0.8167 (1.1196)  time: 0.7521  data: 0.0012  max mem: 27668
[14:09:21.458480] Epoch: [0]  [ 240/1251]  eta: 0:14:04  lr: 0.000092  loss: 0.7759 (1.0916)  time: 0.8565  data: 0.0008  max mem: 27668
[14:09:36.787394] Epoch: [0]  [ 260/1251]  eta: 0:13:42  lr: 0.000100  loss: 0.7625 (1.0666)  time: 0.7664  data: 0.0013  max mem: 27668
[14:09:51.312712] Epoch: [0]  [ 280/1251]  eta: 0:13:19  lr: 0.000107  loss: 0.7480 (1.0441)  time: 0.7262  data: 0.0087  max mem: 27668
[14:10:05.912322] Epoch: [0]  [ 300/1251]  eta: 0:12:56  lr: 0.000115  loss: 0.7449 (1.0243)  time: 0.7299  data: 0.0010  max mem: 27668
[14:10:21.956053] Epoch: [0]  [ 320/1251]  eta: 0:12:39  lr: 0.000123  loss: 0.7312 (1.0060)  time: 0.8021  data: 0.0008  max mem: 27668
[14:10:36.476635] Epoch: [0]  [ 340/1251]  eta: 0:12:18  lr: 0.000130  loss: 0.7212 (0.9895)  time: 0.7259  data: 0.0082  max mem: 27668
[14:10:50.764979] Epoch: [0]  [ 360/1251]  eta: 0:11:56  lr: 0.000138  loss: 0.7098 (0.9741)  time: 0.7053  data: 0.0011  max mem: 27668
[14:11:06.688301] Epoch: [0]  [ 380/1251]  eta: 0:11:40  lr: 0.000146  loss: 0.7020 (0.9601)  time: 0.7954  data: 0.0025  max mem: 27668
[14:11:22.645159] Epoch: [0]  [ 400/1251]  eta: 0:11:24  lr: 0.000153  loss: 0.6927 (0.9469)  time: 0.7978  data: 0.0015  max mem: 27668
[14:11:37.595397] Epoch: [0]  [ 420/1251]  eta: 0:11:05  lr: 0.000161  loss: 0.6783 (0.9344)  time: 0.7474  data: 0.0014  max mem: 27668
[14:11:54.360052] Epoch: [0]  [ 440/1251]  eta: 0:10:51  lr: 0.000169  loss: 0.6914 (0.9233)  time: 0.8381  data: 0.0167  max mem: 27668
[14:12:09.566359] Epoch: [0]  [ 460/1251]  eta: 0:10:33  lr: 0.000176  loss: 0.6812 (0.9129)  time: 0.7603  data: 0.0095  max mem: 27668
[14:12:23.443629] Epoch: [0]  [ 480/1251]  eta: 0:10:14  lr: 0.000184  loss: 0.6902 (0.9035)  time: 0.6938  data: 0.0094  max mem: 27668
[14:12:38.154500] Epoch: [0]  [ 500/1251]  eta: 0:09:56  lr: 0.000192  loss: 0.6957 (0.8952)  time: 0.7355  data: 0.0235  max mem: 27668
[14:12:53.004364] Epoch: [0]  [ 520/1251]  eta: 0:09:39  lr: 0.000200  loss: 0.6954 (0.8876)  time: 0.7424  data: 0.0159  max mem: 27668
[14:13:07.267971] Epoch: [0]  [ 540/1251]  eta: 0:09:21  lr: 0.000207  loss: 0.6927 (0.8804)  time: 0.7131  data: 0.0007  max mem: 27668
[14:13:26.515896] Epoch: [0]  [ 560/1251]  eta: 0:09:09  lr: 0.000215  loss: 0.6806 (0.8733)  time: 0.9623  data: 0.0102  max mem: 27668
[14:13:40.113016] Epoch: [0]  [ 580/1251]  eta: 0:08:51  lr: 0.000223  loss: 0.6882 (0.8669)  time: 0.6798  data: 0.0013  max mem: 27668
[14:13:56.480560] Epoch: [0]  [ 600/1251]  eta: 0:08:35  lr: 0.000230  loss: 0.6920 (0.8611)  time: 0.8178  data: 0.0007  max mem: 27668
[14:14:11.338941] Epoch: [0]  [ 620/1251]  eta: 0:08:18  lr: 0.000238  loss: 0.6747 (0.8553)  time: 0.7428  data: 0.0025  max mem: 27668
[14:14:26.590641] Epoch: [0]  [ 640/1251]  eta: 0:08:02  lr: 0.000246  loss: 0.6907 (0.8502)  time: 0.7625  data: 0.0102  max mem: 27668
[14:14:42.360028] Epoch: [0]  [ 660/1251]  eta: 0:07:46  lr: 0.000253  loss: 0.6954 (0.8455)  time: 0.7877  data: 0.0007  max mem: 27668
[14:14:56.767525] Epoch: [0]  [ 680/1251]  eta: 0:07:29  lr: 0.000261  loss: 0.6806 (0.8407)  time: 0.7203  data: 0.0006  max mem: 27668
[14:15:12.267043] Epoch: [0]  [ 700/1251]  eta: 0:07:13  lr: 0.000269  loss: 0.6771 (0.8360)  time: 0.7749  data: 0.0110  max mem: 27668
[14:15:27.071927] Epoch: [0]  [ 720/1251]  eta: 0:06:57  lr: 0.000276  loss: 0.6895 (0.8320)  time: 0.7402  data: 0.0183  max mem: 27668
[14:15:43.402371] Epoch: [0]  [ 740/1251]  eta: 0:06:42  lr: 0.000284  loss: 0.6803 (0.8279)  time: 0.8164  data: 0.0068  max mem: 27668
[14:15:58.558891] Epoch: [0]  [ 760/1251]  eta: 0:06:25  lr: 0.000292  loss: 0.6818 (0.8241)  time: 0.7578  data: 0.0126  max mem: 27668
[14:16:14.017683] Epoch: [0]  [ 780/1251]  eta: 0:06:10  lr: 0.000299  loss: 0.6858 (0.8206)  time: 0.7729  data: 0.0078  max mem: 27668
[14:16:26.710400] Epoch: [0]  [ 800/1251]  eta: 0:05:52  lr: 0.000307  loss: 0.6714 (0.8168)  time: 0.6346  data: 0.0065  max mem: 27668
[14:16:40.059477] Epoch: [0]  [ 820/1251]  eta: 0:05:35  lr: 0.000315  loss: 0.6745 (0.8134)  time: 0.6674  data: 0.0007  max mem: 27668
[14:16:54.793159] Epoch: [0]  [ 840/1251]  eta: 0:05:19  lr: 0.000322  loss: 0.6872 (0.8104)  time: 0.7366  data: 0.1002  max mem: 27668
[14:17:11.057536] Epoch: [0]  [ 860/1251]  eta: 0:05:04  lr: 0.000330  loss: 0.6812 (0.8075)  time: 0.8132  data: 0.1321  max mem: 27668
[14:17:23.131255] Epoch: [0]  [ 880/1251]  eta: 0:04:47  lr: 0.000338  loss: 0.6756 (0.8045)  time: 0.6036  data: 0.0026  max mem: 27668
[14:17:36.283411] Epoch: [0]  [ 900/1251]  eta: 0:04:31  lr: 0.000345  loss: 0.6728 (0.8016)  time: 0.6576  data: 0.0018  max mem: 27668
[14:17:50.505687] Epoch: [0]  [ 920/1251]  eta: 0:04:15  lr: 0.000353  loss: 0.6744 (0.7989)  time: 0.7110  data: 0.0012  max mem: 27668
[14:18:04.142835] Epoch: [0]  [ 940/1251]  eta: 0:03:59  lr: 0.000361  loss: 0.6861 (0.7967)  time: 0.6818  data: 0.0016  max mem: 27668
[14:18:19.076050] Epoch: [0]  [ 960/1251]  eta: 0:03:43  lr: 0.000368  loss: 0.6779 (0.7942)  time: 0.7466  data: 0.0433  max mem: 27668
[14:18:31.949554] Epoch: [0]  [ 980/1251]  eta: 0:03:27  lr: 0.000376  loss: 0.6709 (0.7917)  time: 0.6436  data: 0.0006  max mem: 27668
[14:18:44.858867] Epoch: [0]  [1000/1251]  eta: 0:03:11  lr: 0.000384  loss: 0.6787 (0.7893)  time: 0.6454  data: 0.0078  max mem: 27668
[14:18:59.548324] Epoch: [0]  [1020/1251]  eta: 0:02:56  lr: 0.000391  loss: 0.6722 (0.7871)  time: 0.7344  data: 0.0778  max mem: 27668
[14:19:12.828216] Epoch: [0]  [1040/1251]  eta: 0:02:40  lr: 0.000399  loss: 0.6648 (0.7848)  time: 0.6639  data: 0.0005  max mem: 27668
[14:19:26.302392] Epoch: [0]  [1060/1251]  eta: 0:02:25  lr: 0.000407  loss: 0.6684 (0.7826)  time: 0.6737  data: 0.0018  max mem: 27668
[14:19:38.974978] Epoch: [0]  [1080/1251]  eta: 0:02:09  lr: 0.000414  loss: 0.6519 (0.7801)  time: 0.6336  data: 0.0064  max mem: 27668
[14:19:54.354367] Epoch: [0]  [1100/1251]  eta: 0:01:54  lr: 0.000422  loss: 0.6493 (0.7777)  time: 0.7689  data: 0.0239  max mem: 27668
[14:20:07.736332] Epoch: [0]  [1120/1251]  eta: 0:01:39  lr: 0.000430  loss: 0.6327 (0.7751)  time: 0.6690  data: 0.0014  max mem: 27668
[14:20:20.691527] Epoch: [0]  [1140/1251]  eta: 0:01:23  lr: 0.000437  loss: 0.6404 (0.7727)  time: 0.6477  data: 0.0119  max mem: 27668
[14:20:33.490741] Epoch: [0]  [1160/1251]  eta: 0:01:08  lr: 0.000445  loss: 0.6293 (0.7703)  time: 0.6399  data: 0.0016  max mem: 27668
[14:20:48.149550] Epoch: [0]  [1180/1251]  eta: 0:00:53  lr: 0.000453  loss: 0.6241 (0.7679)  time: 0.7329  data: 0.0071  max mem: 27668
[14:21:01.380770] Epoch: [0]  [1200/1251]  eta: 0:00:38  lr: 0.000460  loss: 0.6223 (0.7656)  time: 0.6615  data: 0.0008  max mem: 27668
[14:21:14.329802] Epoch: [0]  [1220/1251]  eta: 0:00:23  lr: 0.000468  loss: 0.6321 (0.7635)  time: 0.6474  data: 0.0011  max mem: 27668
[14:21:26.988391] Epoch: [0]  [1240/1251]  eta: 0:00:08  lr: 0.000476  loss: 0.6443 (0.7617)  time: 0.6329  data: 0.0080  max mem: 27668
[14:21:31.329394] Epoch: [0]  [1250/1251]  eta: 0:00:00  lr: 0.000479  loss: 0.6425 (0.7607)  time: 0.5328  data: 0.0014  max mem: 27668
[14:21:32.219841] Epoch: [0] Total time: 0:15:32 (0.7452 s / it)
[14:21:32.240455] Averaged stats: lr: 0.000479  loss: 0.6425 (0.7611)
